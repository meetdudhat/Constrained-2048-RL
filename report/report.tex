\documentclass{article}

% --- NeurIPS Style ---
\usepackage[preprint]{neurips_2023}

% --- Packages ---
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}

% --- Title ---
\title{Reinforcement Learning for Constrained 2048: \\ A Comparative Study of Deep Q-Networks and AlphaZero}

% --- Authors ---
\author{%
  Ayman Madani \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aymanmadani@cmail.carleton.ca} \\
  \And
  Jason Xu \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{jasonxu@cmail.carleton.ca} \\
  \And
  Sachin Bansal \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{sachinbansal@cmail.carleton.ca} \\
  \And
  Meetkumar Dudhat \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{meetkumardudhat@cmail.carleton.ca} \\
  \And
  Arya Shah \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aryashah@cmail.carleton.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This project investigates the effectiveness of Model-Free versus Model-Based Reinforcement Learning in solving the stochastic puzzle game 2048, with a specific focus on a ``Constrained'' variant containing an immovable obstacle. We implement a baseline Deep Q-Network (DQN) using Stable Baselines3 and compare it against a custom AlphaZero implementation featuring a Monte Carlo Tree Search (MCTS) engine and a Hybrid-Kernel Convolutional Neural Network. Empirical results demonstrate that while the reactive DQN struggles to adapt to the constrained environment (Average Score: 1,226), the deliberative AlphaZero agent successfully plans around the obstacle (Average Score: 2,231), achieving a 1.8x performance improvement. The study confirms that lookahead search is essential for navigating constrained stochastic domains.
\end{abstract}

% ==========================================
% SECTION 1: INTRODUCTION
% ==========================================
\section{Introduction}
\label{sec:intro}

The game 2048 is often used to study reinforcement learning algorithms because it contains a mix of randomness and requires long-term planning. Although it looks simple at first, it actually has a large number of possible board states (on the order of $10^{16}$). This game requires an agent to think about immediate merges for points together with long term board structure so it does not get stuck later. Most existing RL agents rely heavily on deterministic human strategies like the ``corner strategy'', where the highest tile is kept in one corner to maintain stability throughout the game.

In many real life decision-making tasks, including variants of 2048, the agent does not always get a perfectly open environment. There may be some blocked areas, fixed obstacles, or parts of the space that cannot be used, which means that common heuristics like the corner strategy do not always hold. To explore this idea further, we examined a tougher version of the 2048 game. In our setup, we placed an immovable block at the coordinate $(3,0)$ on a standard $4 \times 4$ board. This block has a value of $-1$ and permanently occupies a grid cell. It never shifts or merges with any other tile. Although it is only a single blocked space, it changes how the entire board evolves because the agent now must work around this obstacle for the entire duration of the game.

This added obstacle immediately disrupts the usual corner-based approach and reduces the safe spaces the agent can use. As a result, the agent must discover different ways to structure the board and can no longer rely on keeping high valued tiles locked in a corner anymore. The decision-making problem becomes significantly harder because the agent needs to reconsider how it forms merges over time, avoid blocked cell, and still manage the randomness of new tiles appearing. Studying this constrained setup helps reveal how both model-free and model-based RL methods behave when the environment is not fully open and imposes permanent spatial limits like the immovable block restricting movement.


\subsection{MDP Specifications}
\label{sec:mdp}
\begin{itemize}
    \item \textbf{State Space ($S$):} A $4 \times 4$ grid where each cell contains a tile value $t \in \{2^1, \dots, 2^{16}\}$ or is empty. In Constrained environment, one fixed coordinate contains an Immovable block with value of -1, reducing reachable grid configurations and altering legal transitions.
    \item \textbf{Action Space ($A$):} A Discrete set of four actions $\{ \text{Up, Down, Left, Right} \}$. Each action slides all tiles in the chosen direction and merges matching tiles.
    \item \textbf{Reward Function ($R$):} The standard 2048 reward is the merge value gained after combining tiles. Sparse binary reward ($+1$ for Win (achieving 2048 tile), $-1$ for Loss (no moves available)).
    \item \textbf{Transition Function ($P$):} After each valid move, a new tile appears in a random location: a $2$ with $0.9$ probability and a $4$ with $0.1$ probability. In the Constrained environment, the Immovable Block is excluded from movement and from tile spawning. \\
    
    This MDP formulation allows us to compare how different RL approaches, specifically a Model-Free DQN and a Model-Based AlphaZero-style agent, adapt to the spatial obstruction, handle restricted transitions, and develop strategies that remain robust under constrained dynamics.
\end{itemize}

% ==========================================
% SECTION 2: PRIOR WORK
% ==========================================
\section{Related Work}
\label{sec:related}
Szepesvári's \textit{Algorithms for Reinforcement Learning} \cite{szepesvari2010} gives us the theory behind value-based methods and was the first to establish PAC bounds on tabular Q-learning and explore the difficulties of function approximation. Our DQN baseline implementation is a DQN version of these classical principles, but operates  in a high-dimensional setting where neural networks approximate $Q(s,a)$. This is exactly the case where Szepesvári's exploration-exploitation paradox is particularly important in the given context and where extra dimensionality complicates things.

Model-free value-based reinforcement learning was popularized by the Deep Q-Network (DQN) algorithm \cite{mnih2015dqn} by Mnih et al., which improved Q-learning on Atari with the help of replay buffers and target networks. Our baseline approach uses essentially the same recipe (replay and target network) on a reduced $4\times4$ grid with tile placement noise and a static obstacle such that generalization is more a result of exploration than image feature generalization.

In Prioritized Experience Replay from Schaul et al. \cite{schaul2016per}, sample efficiency is increased by replaying transitions with high TDerror. For this project, we chose to only implement uniform replay; however, with the potential benefits of replay priorities, fewer episodes may be necessary for obstacle-aware policies.

Rainbow DQN \cite{hessel2018rainbow} combines six extensions (Double Q-learning, PER, multi-step return, dueling networks with multi-output heads, Noisy Nets, distributional RL) and achieves significant Atari improvements. Our baseline excludes these for being simpler; the multi-step return and Noisy Nets from Rainbow might assist with sparse rewards from the game's merge action and continued exploration after the replay buffer is full.

AlphaZero by Silver et al. \cite{silver2018azero} integrates policy-value networks with Monte Carlo Tree Search (MCTS) for perfect information game play. Our model-based agent adopts this planning approach and modifies the application scope of MCTS to a stochastic single-player environment with chance nodes for tile appearances and hybrid kernels for examining full rows and columns, thus explaining its strength within the limited environment that requires significant planning.

Gayas (Expectimax for 2048) \cite{gayas2014} implement depth-limited searches with carefully designed features involving the ability to merge/monotonicity. In comparison to the above-mentioned traditional depth-limited searches as baselines, while the DQN learns to estimate values from experience instead and lacks any form of lookahead, which degrades when the immovable block breaks the standard cornering heuristics.

Antonoglou et al.\ \cite{antonoglou2022} investigate planning with learnable models in stochastic tasks. Their observations confirm our finding: AlphaZero-style model-based rollouts perform better than reactive value-based approaches if one has to plan for uncertainty about the future (tile appearance) and structural constraints (cell occupied).
% ==========================================
% SECTION 3: APPROACHES
% ==========================================
\section{Approaches}
\label{sec:approach}

\subsection{Baseline: Deep Q-Network (DQN)}
We apply a standard DQN from Stable Baselines3 with an MLP policy (two layers with 256-dimensional hidden representations) on a \texttt{Log2}-normalized $4\times4$ game board. Our agent is purely reactive with no forward thinking or search. Important hyperparameters included, with some variations between training, a replay buffer size of 100K transitions, batch size of 128, target network update frequency at every 1K steps, and learning rate at $10^{-4}$ with $\varepsilon$-greedy exploration schedule decaying to $0.02$ over training time. We optimize with unprocessed game merge rewards (or log-merge for ablated experiments). It is a lightweight method, unable to play well when the obstacle obstructs the corner showing the difficulty with model-free methods within spatial constraints and for uncertain game start locations.

\subsection{Improved: AlphaZero (MCTS + Hybrid CNN)}
To address the limitations of reactive policies in the Constrained environment, we implemented a variation of the AlphaZero algorithm adapted for single-player stochastic domains. The architecture consists of two primary components: a Monte Carlo Tree Search (MCTS) engine for planning and a Deep Neural Network for intuition.

\subsubsection{Stochastic MCTS Engine}
Unlike the standard AlphaZero algorithm which assumes an adversarial opponent (Minimax), our MCTS models the environment as a stochastic actor. The search tree alternates between two node types:
\begin{itemize}
    \item \textbf{Decision Nodes:} Represent states where the agent selects an action $a \in A$. Selection follows the Predictor + Upper Confidence Bound (PUCT) formula to balance exploration and exploitation.
    \item \textbf{Chance Nodes:} Represent states where the environment spawns a tile. These nodes do not use UCB but instead expand children based on the transition probabilities ($0.9$ for a 2, $0.1$ for a 4).
\end{itemize}
This distinction allows the agent to calculate the \textit{expected value} of a move over 150 future simulations, effectively ``imagining'' the board filling up around the obstacle.

\subsubsection{Hybrid-Kernel Neural Network}
Standard Convolutional Neural Networks (CNNs) utilize $3 \times 3$ filters, which are optimized for local spatial features in images. However, 2048 mechanics rely on sliding entire rows and columns. To capture this, we designed a Hybrid-Kernel architecture:
\begin{itemize}
    \item \textbf{$1 \times 4$ Kernels:} Scan entire horizontal rows to detect merge potential and obstructions.
    \item \textbf{$4 \times 1$ Kernels:} Scan entire vertical columns.
\end{itemize}
This architectural choice provides the agent with the global context necessary to detect if a ``snake'' chain is broken by the immovable block in the constrained environment.

\subsubsection{State Representation \& Optimization}
We addressed the exponential scale of tile values (2 vs 65,536) by implementing a \texttt{Log2} normalization wrapper, converting values to a linear scale ($1 \dots 16$). Furthermore, to support the computational demands of MCTS, the game physics engine was optimized using Numba JIT compilation, accelerating simulation speed by approximately $50\times$.

% ==========================================
% SECTION 4: EMPIRICAL STUDIES
% ==========================================
\section{Empirical Studies}
\label{sec:experiments}

\subsection{Experimental Setup}
We evaluated both agents in two environments:
\begin{enumerate}
    \item \textbf{Standard:} Classic 2048 rules.
    \item \textbf{Constrained:} An immovable block (value $-1$) is placed at coordinate $(3,0)$.
\end{enumerate}
Evaluation metrics include Average Score, Max Tile Reached, and Win Rate over 100 test episodes.

\subsection{Main Results}
Table \ref{tab:results} summarizes the performance comparison.


\begin{table}[h]
    \centering
    \caption{Performance comparison over 100 episodes.}
    \label{tab:results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Environment} & \textbf{Agent} & \textbf{Avg Score} & \textbf{Max Tile (Avg)} & \textbf{Improvement} \\
        \midrule
        Standard & Baseline (DQN) & 2,096 & 187 & - \\
        Standard & \textbf{AlphaZero (Ours)} & \textbf{7,371} & \textbf{560} & \textbf{+251\%} \\
        \midrule
        Constrained & Baseline (DQN) & 1,226 & 129 & - \\
        Constrained & \textbf{AlphaZero (Ours)} & \textbf{2,231} & \textbf{202} & \textbf{+82\%} \\
        \bottomrule
    \end{tabular}
\end{table}

The results indicate a decisive advantage for the Model-Based approach. In the Standard environment, AlphaZero achieved an average score $3.5\times$ higher than the Baseline. More importantly, in the \textbf{Constrained} environment, the Baseline's performance degraded by $41\%$ as it failed to adapt to the dead zone. The AlphaZero agent, utilizing lookahead search, successfully navigated the constraint, maintaining a strong average score of 2,231. Qualitative analysis of the final board states reveals that the AlphaZero agent learned to anchor its tile chain in the corner \textit{opposite} the immovable block, a strategic adaptation that the reactive Baseline failed to discover.

\subsection{Ablation Studies}
\label{sec:ablation}


To understand the influence of state representation on learning stability, we conducted an ablation comparing two encodings of the 2048 board: (1) Raw tile values, where entries range exponentially from 2 to 65{,}536, and (2) a Log$_2$ encoding that compresses all tiles into the range 1--16. When trained with Raw values, the AlphaZero agent achieved an average score of only 1{,}025 and frequently produced erratic value estimates. The sharp numerical disparity between small and large tiles caused unstable gradients in the Hybrid-Kernel CNN, reducing the reliability of MCTS rollouts.


\includegraphics[width = 1 \textwidth]{images/raw_vs_log.png}

In contrast, the Log$_2$ representation yielded an average score of 2{,}096, nearly doubling performance. This improvement stems from transforming the exponentially scaled tile distribution into a linear space that is easier for the network to model. The normalized representation allowed the CNN to learn merge patterns more consistently and enabled MCTS to propagate value estimates more accurately across simulated trajectories. Overall, our ablation confirms that normalization is not merely beneficial but essential for robust policy learning in 2048-like domains.


\subsection{Complexity Analysis}


Although both agents operate in the same environment, their computational demands differ significantly. The Baseline DQN required approximately 30 minutes of training, driven largely by simple forward passes through a lightweight MLP and off-policy updates using batches from the replay buffer. In contrast, the AlphaZero pipeline required close to 10 hours to train under comparable evaluation conditions. The principal source of overhead is Monte Carlo Tree Search: each move requires 150 simulations, each simulation invoking multiple forward passes through the Hybrid-Kernel CNN.

\includegraphics[width = 1 \textwidth]{images/tarining_time.png}

Additional cost arises from processing Chance Nodes to model stochastic tile spawning, which further expands the number of states evaluated during planning. Despite this expense, the resulting policy is substantially stronger and far more adaptive, particularly in the Constrained environment where long-term spatial reasoning is required. Importantly, while training is costly, inference after training is efficient and requires only a single network evaluation per move. This reflects a classic trade-off in reinforcement learning: higher upfront computation yields significantly improved decision quality.
% ==========================================
% SECTION 5: CONCLUSION
% ==========================================
\section{Conclusion}
\label{sec:conclusion}
\subsection{What we learned about the problem}

We started our project with a seemingly simple puzzle 2048, but the introduction of an immovable, non-merging "$-$1" tile turned it into a much richer reinforcement learning and planning problem. In the standard game, many strong strategies are essentially short-sighted but geometry-aware: keeping the largest tile in a corner, preserving monotonic rows and columns, and greedily taking merges that maintain empty space. In our constrained variant, these rules of thumb are no longer reliably safe. A single push that funnels mass toward the blocked corner can irreversibly destroy board geometry, trapping large tiles behind the dead zone, and collapsing future movability.

This forced us to confront two important aspects of the problem:

\begin{enumerate}
    \item Long-horizon credit assignment: moves that look harmless but can be catastrophic several steps later, making short-term reward maximization insufficient.

    \item Structure awareness: the agent must understand the board's physics, that is, how the constraint interacts with tile flows and empty cells rather than just memorize local patterns.
\end{enumerate}

What we learned is that the structural constraints transform 2048 from a greedy-friendly game into a planning heavy control problem.

\subsection{What we learned about the approaches}

We implemented and compared deep RL baselines algorithms like DQN-style methods in our custom constrained environment with a model-based AlphaZero style agent using MCTS. This comparison provided us with several insights:

\begin{itemize}
    \item Model-free deep RL conflict under constraints, even with sensible state encodings and reward shaping, the DQN baselines tended to overvalue short-term merges and often drifted tiles toward the blocked corner. They could achieve reasonable scores in easier settings, but in the truly constrained scenario they frequently converged to locally consistent, failing to systematically avoid the dead zone.
    \item Model-based RL handled the constraint more gracefully. The AlphaZero agent explicitly simulates future roll-outs before committing to an action, allowing it to see that certain moves will squeeze tiles into unsalvageable positions. In qualitative implementations, it was observed that the planning agent tended to maintain a clean region away from the $-$1 block, keeping high-value tiles in safer lanes and maintaining more empty cells over time. 

\end{itemize}

We realized that the constrained 2048 problem is not only "difficult 2048", but is fundamentally a testbed for the value of look-ahead and for the limits of model-free deep RL.

\subsection{Limitations and Failure Modes Observed}

Despite the advantages of search, our system can still fail when stochastic spawns align unfavorably, or when the search budget is too small to notice rare but catastrophic futures (e.g., a merge that looks good but collapses the safe stack). On the model-free side, we observed sensitivity to reward shaping and replay content; agents could learn locally stable but globally brittle strategies if evaluation emphasized short horizons. 

\subsection{How the method can be improved with more time and resources}

\begin{enumerate}
    \item Train for longer and extend the scale search
            Run more self-play training for the AlphaZero agent and allocate more MCTS simulations per move, especially in late game positions where the constraints are more dangerous. This might sharpen the policy's understanding of catastrophic futures and improve robustness.

    \item Introduce dynamic constraints
            So far the constraint is static. A natural next step can be dynamic constraints, where we progressively increase constraint difficulty during training. This would test generalization and encourage the agent to learn more abstract principles like avoiding high-value tiles isolation rather than over-lifting to one specific layout.

    \item Use a deeper ResNet Backbone
            A deeper ResNet could better capture the global board structure such as empty-cell patterns around the $-$1 block and provide policy estimates for MCTS to build on.

Beyond these primary directions, hybrid approaches can also be considered, with distributional RL for risk aware planning and systematic studies on reward shaping.

\end{enumerate}

\subsection{Summary}

In this project, we designed a constrained 2048 environment, implemented model-free deep RL baselines and an AlphaZero-style MCTS agent, and systematically compared how they behave under structural constraints. 

We learned that:

\begin{itemize}
    \item The constraint fundamentally changes the nature of the task, making long-term planning and geometry preservation essential.

    \item Standard deep RL, which can perform reasonably in unconstrained 2048, often fails to internalize the consequences of the dead zone.

    \item Model-based RL with MCTS is far better suited to this setting, precisely because it can simulate the environment’s physics and choose actions that remain viable many steps into the future.


\end{itemize}

The most promising directions are to train for longer, upgrade to a deeper ResNet backbone, and explore dynamic constraints that further stress-test the agent’s ability to generalize. More broadly, our findings highlight a transferable lesson for real-world RL: when the environment contains hard constraints and irreversible failures, planning and model-based reasoning are often necessary for robust performance. 

 
% --- References ---
\medskip
\small
\begin{thebibliography}{9}

% Add other references here
\bibitem{antonoglou2022}
Antonoglou, I., et al. (2022). Planning in Stochastic Environments with a Learned Model. \textit{International Conference on Learning Representations (ICLR)}. Available at: \url{https://openreview.net/pdf?id=X6D9bAHhBQ1}

\bibitem{fuchs2020}
Fuchs, S. (2020). \textit{2048 AI | Develop \& Fix}. Available at: \url{https://www.sam-fuchs.com/s/projects/2048/}

\bibitem{gayas2014}
Gayas, V. (2014). \textit{2048 Using Expectimax}. Miner School of Computer and Information Sciences, UMass Lowell. Available at: \url{https://www.cs.uml.edu/ecg/uploads/AIfall14/vignesh_gayas_2048_project.pdf}

\bibitem{hessel2018rainbow}
Hessel, M., Modayil, J., Van Hasselt, H., et al. (2018). \textit{Rainbow: Combining improvements in deep reinforcement learning}. In AAAI Conference on Artificial Intelligence (AAAI). https://arxiv.org/abs/1710.02298 

\bibitem{mnih2015dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015).  \textit{Human-level control through deep reinforcement learning}. Nature. https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf 

\bibitem{schaul2016per}
Schaul, T., Quan, J., Antonoglou, I., \& Silver, D. (2016). \textit{Prioritized experience replay}. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/1511.05952 

\bibitem{silver2018azero}
Silver, D., Hubert, T., Schrittwieser, J., et al. (2018). \textit{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}. Science. https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero\_preprint.pdf 

\bibitem{szepesvari2010}
Szepesvári, C. (2010). \textit{Algorithms for reinforcement learning}. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan \& Claypool Publishers. https://doi.org/10.2200/S00268ED1V01Y201005AIM009 
\end{thebibliography}

\end{document}
