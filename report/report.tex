\documentclass{article}

% --- NeurIPS Style ---
\usepackage[preprint]{neurips_2023}

% --- Packages ---
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}

% --- Title ---
\title{Reinforcement Learning for Constrained 2048: \\ A Comparative Study of Deep Q-Networks and AlphaZero}

% --- Authors ---
\author{%
  Ayman Madani \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aymanmadani@cmail.carleton.ca} \\
  \And
  Jason Xu \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{jasonxu@cmail.carleton.ca} \\
  \And
  Sachin Bansal \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{sachinbansal@cmail.carleton.ca} \\
  \And
  Meetkumar Dudhat \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{meetkumardudhat@cmail.carleton.ca} \\
  \And
  Arya Shah \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aryashah@cmail.carleton.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This project investigates the effectiveness of Model-Free versus Model-Based Reinforcement Learning in solving the stochastic puzzle game 2048, with a specific focus on a ``Constrained'' variant containing an immovable obstacle. We implement a baseline Deep Q-Network (DQN) using Stable Baselines3 and compare it against a custom AlphaZero implementation featuring a Monte Carlo Tree Search (MCTS) engine and a Hybrid-Kernel Convolutional Neural Network. Empirical results demonstrate that while the reactive DQN struggles to adapt to the constrained environment (Average Score: 1,226), the deliberative AlphaZero agent successfully plans around the obstacle (Average Score: 2,231), achieving a 1.8x performance improvement. The study confirms that lookahead search is essential for navigating constrained stochastic domains.
\end{abstract}

% ==========================================
% SECTION 1: INTRODUCTION
% ==========================================
\section{Introduction}
\label{sec:intro}

\textit{[Member 2: Define the problem. Mention that 2048's a state space. Explain why this is hard. Introduce the "Constrained" environment (immovable block at 3,0) and why it breaks standard strategies.]}

\subsection{MDP Specifications}
\label{sec:mdp}
\textit{[Member 2: Formalize the Standard Markov Decision Process here. PLEASE NOTE: Add the definition of the original \textbf{Standard Reward} (points for merging) here vs the improved MDP]}
\begin{itemize}
    \item \textbf{State Space ($S$):} A $4 \times 4$ grid where each tile $t \in \{2^1, \dots, 2^{16}\}$.
    \item \textbf{Action Space ($A$):} Discrete set $\{ \text{Up, Down, Left, Right} \}$.
    \item \textbf{Reward Function ($R$):} Sparse binary reward ($+1$ for Win, $-1$ for Loss).
    \item \textbf{Transitions ($P$):} Stochastic tile spawning ($90\%$ prob of 2, $10\%$ prob of 4).
\end{itemize}

% ==========================================
% SECTION 2: PRIOR WORK
% ==========================================
\section{Related Work}
\label{sec:related}

\textit{[Member 3: Summarize at least 5 papers. Compare them to our work. Add references.]}

% ==========================================
% SECTION 3: APPROACHES
% ==========================================
\section{Approaches}
\label{sec:approach}

\subsection{Baseline: Deep Q-Network (DQN)}
\textit{[Member 3: Describe the Baseline. Mention Stable Baselines3, MLP Policy, and the lack of lookahead.]}

\subsection{Improved: AlphaZero (MCTS + Hybrid CNN)}
To address the limitations of reactive policies in the Constrained environment, we implemented a variation of the AlphaZero algorithm adapted for single-player stochastic domains. The architecture consists of two primary components: a Monte Carlo Tree Search (MCTS) engine for planning and a Deep Neural Network for intuition.

\subsubsection{Stochastic MCTS Engine}
Unlike the standard AlphaZero algorithm which assumes an adversarial opponent (Minimax), our MCTS models the environment as a stochastic actor. The search tree alternates between two node types:
\begin{itemize}
    \item \textbf{Decision Nodes:} Represent states where the agent selects an action $a \in A$. Selection follows the Predictor + Upper Confidence Bound (PUCT) formula to balance exploration and exploitation.
    \item \textbf{Chance Nodes:} Represent states where the environment spawns a tile. These nodes do not use UCB but instead expand children based on the transition probabilities ($0.9$ for a 2, $0.1$ for a 4).
\end{itemize}
This distinction allows the agent to calculate the \textit{expected value} of a move over 150 future simulations, effectively ``imagining'' the board filling up around the obstacle.

\subsubsection{Hybrid-Kernel Neural Network}
Standard Convolutional Neural Networks (CNNs) utilize $3 \times 3$ filters, which are optimized for local spatial features in images. However, 2048 mechanics rely on sliding entire rows and columns. To capture this, we designed a Hybrid-Kernel architecture:
\begin{itemize}
    \item \textbf{$1 \times 4$ Kernels:} Scan entire horizontal rows to detect merge potential and obstructions.
    \item \textbf{$4 \times 1$ Kernels:} Scan entire vertical columns.
\end{itemize}
This architectural choice provides the agent with the global context necessary to detect if a ``snake'' chain is broken by the immovable block in the constrained environment.

\subsubsection{State Representation \& Optimization}
We addressed the exponential scale of tile values (2 vs 65,536) by implementing a \texttt{Log2} normalization wrapper, converting values to a linear scale ($1 \dots 16$). Furthermore, to support the computational demands of MCTS, the game physics engine was optimized using Numba JIT compilation, accelerating simulation speed by approximately $50\times$.

% ==========================================
% SECTION 4: EMPIRICAL STUDIES
% ==========================================
\section{Empirical Studies}
\label{sec:experiments}

\subsection{Experimental Setup}
We evaluated both agents in two environments:
\begin{enumerate}
    \item \textbf{Standard:} Classic 2048 rules.
    \item \textbf{Constrained:} An immovable block (value $-1$) is placed at coordinate $(3,0)$.
\end{enumerate}
Evaluation metrics include Average Score, Max Tile Reached, and Win Rate over 100 test episodes.

\subsection{Main Results}
Table \ref{tab:results} summarizes the performance comparison.


\begin{table}[h]
    \centering
    \caption{Performance comparison over 100 episodes.}
    \label{tab:results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Environment} & \textbf{Agent} & \textbf{Avg Score} & \textbf{Max Tile (Avg)} & \textbf{Improvement} \\
        \midrule
        Standard & Baseline (DQN) & 2,096 & 187 & - \\
        Standard & \textbf{AlphaZero (Ours)} & \textbf{7,371} & \textbf{560} & \textbf{+251\%} \\
        \midrule
        Constrained & Baseline (DQN) & 1,226 & 129 & - \\
        Constrained & \textbf{AlphaZero (Ours)} & \textbf{2,231} & \textbf{202} & \textbf{+82\%} \\
        \bottomrule
    \end{tabular}
\end{table}

The results indicate a decisive advantage for the Model-Based approach. In the Standard environment, AlphaZero achieved an average score $3.5\times$ higher than the Baseline. More importantly, in the \textbf{Constrained} environment, the Baseline's performance degraded by $41\%$ as it failed to adapt to the dead zone. The AlphaZero agent, utilizing lookahead search, successfully navigated the constraint, maintaining a strong average score of 2,231. Qualitative analysis of the final board states reveals that the AlphaZero agent learned to anchor its tile chain in the corner \textit{opposite} the immovable block, a strategic adaptation that the reactive Baseline failed to discover.

\subsection{Ablation Studies}
\label{sec:ablation}
\textit{[Member 4: Discuss Log2 vs Raw Values. Use the data: Log2 Avg Score (2096) vs Raw Values Avg Score (1025). Conclusion: Normalization is critical.]}

\subsection{Complexity Analysis}
\textit{[Member 4: Compare training times. AlphaZero took ~10 hours vs Baseline ~30 mins. Discuss the trade-off between training cost and inference quality.]}

% ==========================================
% SECTION 5: CONCLUSION
% ==========================================
\section{Conclusion}
\label{sec:conclusion}
\textit{[Member 5: Summarize the project. What did we learn? Future work: Train for longer, use deeper ResNet, dynamic constraints.]}

% --- References ---
\medskip
\small
\begin{thebibliography}{9}

% Add other references here
\bibitem{gayas2014}
Gayas, V. (2014). \textit{2048 Using Expectimax}. Miner School of Computer and Information Sciences, UMass Lowell. Available at: \url{https://www.cs.uml.edu/ecg/uploads/AIfall14/vignesh_gayas_2048_project.pdf}

\bibitem{fuchs2020}
Fuchs, S. (2020). \textit{2048 AI | Develop \& Fix}. Available at: \url{https://www.sam-fuchs.com/s/projects/2048/}

\bibitem{antonoglou2022}
Antonoglou, I., et al. (2022). Planning in Stochastic Environments with a Learned Model. \textit{International Conference on Learning Representations (ICLR)}. Available at: \url{https://openreview.net/pdf?id=X6D9bAHhBQ1}

\end{thebibliography}

\end{document}