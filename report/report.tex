\documentclass{article}

% --- NeurIPS Style ---
\usepackage[preprint]{neurips_2023}

% --- Packages ---
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}

% --- Title ---
\title{Reinforcement Learning for Constrained 2048: \\ A Comparative Study of Deep Q-Networks and AlphaZero}

% --- Authors ---
\author{%
  Ayman Madani \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aymanmadani@cmail.carleton.ca} \\
  \And
  Jason Xu \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{jasonxu@cmail.carleton.ca} \\
  \And
  Sachin Bansal \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{sachinbansal@cmail.carleton.ca} \\
  \And
  Meetkumar Dudhat \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{meetkumardudhat@cmail.carleton.ca} \\
  \And
  Arya Shah \\
  Department of Computer Science\\
  Carleton University\\
  \texttt{aryashah@cmail.carleton.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This project investigates the effectiveness of Model-Free versus Model-Based Reinforcement Learning in solving the stochastic puzzle game 2048, with a specific focus on a ``Constrained'' variant containing an immovable obstacle. We implement a baseline Deep Q-Network (DQN) using Stable Baselines3 and compare it against a custom AlphaZero implementation featuring a Monte Carlo Tree Search (MCTS) engine and a Hybrid-Kernel Convolutional Neural Network. Empirical results demonstrate that while the reactive DQN struggles to adapt to the constrained environment (Average Score: 1,226), the deliberative AlphaZero agent successfully plans around the obstacle (Average Score: 2,231), achieving a 1.8x performance improvement. The study confirms that lookahead search is essential for navigating constrained stochastic domains.
\end{abstract}

% ==========================================
% SECTION 1: INTRODUCTION
% ==========================================
\section{Introduction}
\label{sec:intro}

\textit{[Member 2: Define the problem. Mention that 2048's a state space. Explain why this is hard. Introduce the "Constrained" environment (immovable block at 3,0) and why it breaks standard strategies.]}

\subsection{MDP Specifications}
\label{sec:mdp}
\textit{[Member 2: Formalize the Standard Markov Decision Process here. PLEASE NOTE: Add the definition of the original \textbf{Standard Reward} (points for merging) here vs the improved MDP]}
\begin{itemize}
    \item \textbf{State Space ($S$):} A $4 \times 4$ grid where each tile $t \in \{2^1, \dots, 2^{16}\}$.
    \item \textbf{Action Space ($A$):} Discrete set $\{ \text{Up, Down, Left, Right} \}$.
    \item \textbf{Reward Function ($R$):} Sparse binary reward ($+1$ for Win, $-1$ for Loss).
    \item \textbf{Transitions ($P$):} Stochastic tile spawning ($90\%$ prob of 2, $10\%$ prob of 4).
\end{itemize}

% ==========================================
% SECTION 2: PRIOR WORK
% ==========================================
\section{Related Work}
\label{sec:related}

\textit{[Member 3: Summarize at least 5 papers. Compare them to our work. Add references.]}

% ==========================================
% SECTION 3: APPROACHES
% ==========================================
\section{Approaches}
\label{sec:approach}

\subsection{Baseline: Deep Q-Network (DQN)}
\textit{[Member 3: Describe the Baseline. Mention Stable Baselines3, MLP Policy, and the lack of lookahead.]}

\subsection{Improved: AlphaZero (MCTS + Hybrid CNN)}
To address the limitations of reactive policies in the Constrained environment, we implemented a variation of the AlphaZero algorithm adapted for single-player stochastic domains. The architecture consists of two primary components: a Monte Carlo Tree Search (MCTS) engine for planning and a Deep Neural Network for intuition.

\subsubsection{Stochastic MCTS Engine}
Unlike the standard AlphaZero algorithm which assumes an adversarial opponent (Minimax), our MCTS models the environment as a stochastic actor. The search tree alternates between two node types:
\begin{itemize}
    \item \textbf{Decision Nodes:} Represent states where the agent selects an action $a \in A$. Selection follows the Predictor + Upper Confidence Bound (PUCT) formula to balance exploration and exploitation.
    \item \textbf{Chance Nodes:} Represent states where the environment spawns a tile. These nodes do not use UCB but instead expand children based on the transition probabilities ($0.9$ for a 2, $0.1$ for a 4).
\end{itemize}
This distinction allows the agent to calculate the \textit{expected value} of a move over 150 future simulations, effectively ``imagining'' the board filling up around the obstacle.

\subsubsection{Hybrid-Kernel Neural Network}
Standard Convolutional Neural Networks (CNNs) utilize $3 \times 3$ filters, which are optimized for local spatial features in images. However, 2048 mechanics rely on sliding entire rows and columns. To capture this, we designed a Hybrid-Kernel architecture:
\begin{itemize}
    \item \textbf{$1 \times 4$ Kernels:} Scan entire horizontal rows to detect merge potential and obstructions.
    \item \textbf{$4 \times 1$ Kernels:} Scan entire vertical columns.
\end{itemize}
This architectural choice provides the agent with the global context necessary to detect if a ``snake'' chain is broken by the immovable block in the constrained environment.

\subsubsection{State Representation \& Optimization}
We addressed the exponential scale of tile values (2 vs 65,536) by implementing a \texttt{Log2} normalization wrapper, converting values to a linear scale ($1 \dots 16$). Furthermore, to support the computational demands of MCTS, the game physics engine was optimized using Numba JIT compilation, accelerating simulation speed by approximately $50\times$.

% ==========================================
% SECTION 4: EMPIRICAL STUDIES
% ==========================================
\section{Empirical Studies}
\label{sec:experiments}

\subsection{Experimental Setup}
We evaluated both agents in two environments:
\begin{enumerate}
    \item \textbf{Standard:} Classic 2048 rules.
    \item \textbf{Constrained:} An immovable block (value $-1$) is placed at coordinate $(3,0)$.
\end{enumerate}
Evaluation metrics include Average Score, Max Tile Reached, and Win Rate over 100 test episodes.

\subsection{Main Results}
Table \ref{tab:results} summarizes the performance comparison.


\begin{table}[h]
    \centering
    \caption{Performance comparison over 100 episodes.}
    \label{tab:results}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Environment} & \textbf{Agent} & \textbf{Avg Score} & \textbf{Max Tile (Avg)} & \textbf{Improvement} \\
        \midrule
        Standard & Baseline (DQN) & 2,096 & 187 & - \\
        Standard & \textbf{AlphaZero (Ours)} & \textbf{7,371} & \textbf{560} & \textbf{+251\%} \\
        \midrule
        Constrained & Baseline (DQN) & 1,226 & 129 & - \\
        Constrained & \textbf{AlphaZero (Ours)} & \textbf{2,231} & \textbf{202} & \textbf{+82\%} \\
        \bottomrule
    \end{tabular}
\end{table}

The results indicate a decisive advantage for the Model-Based approach. In the Standard environment, AlphaZero achieved an average score $3.5\times$ higher than the Baseline. More importantly, in the \textbf{Constrained} environment, the Baseline's performance degraded by $41\%$ as it failed to adapt to the dead zone. The AlphaZero agent, utilizing lookahead search, successfully navigated the constraint, maintaining a strong average score of 2,231. Qualitative analysis of the final board states reveals that the AlphaZero agent learned to anchor its tile chain in the corner \textit{opposite} the immovable block, a strategic adaptation that the reactive Baseline failed to discover.

\subsection{Ablation Studies}
\label{sec:ablation}


To understand the influence of state representation on learning stability, we conducted an ablation comparing two encodings of the 2048 board: (1) Raw tile values, where entries range exponentially from 2 to 65{,}536, and (2) a Log$_2$ encoding that compresses all tiles into the range 1--16. When trained with Raw values, the AlphaZero agent achieved an average score of only 1{,}025 and frequently produced erratic value estimates. The sharp numerical disparity between small and large tiles caused unstable gradients in the Hybrid-Kernel CNN, reducing the reliability of MCTS rollouts.


\includegraphics[width = 1 \textwidth]{images/raw_vs_log.png}

In contrast, the Log$_2$ representation yielded an average score of 2{,}096, nearly doubling performance. This improvement stems from transforming the exponentially scaled tile distribution into a linear space that is easier for the network to model. The normalized representation allowed the CNN to learn merge patterns more consistently and enabled MCTS to propagate value estimates more accurately across simulated trajectories. Overall, our ablation confirms that normalization is not merely beneficial but essential for robust policy learning in 2048-like domains.


\subsection{Complexity Analysis}


Although both agents operate in the same environment, their computational demands differ significantly. The Baseline DQN required approximately 30 minutes of training, driven largely by simple forward passes through a lightweight MLP and off-policy updates using batches from the replay buffer. In contrast, the AlphaZero pipeline required close to 10 hours to train under comparable evaluation conditions. The principal source of overhead is Monte Carlo Tree Search: each move requires 150 simulations, each simulation invoking multiple forward passes through the Hybrid-Kernel CNN.

\includegraphics[width = 1 \textwidth]{images/tarining_time.png}

Additional cost arises from processing Chance Nodes to model stochastic tile spawning, which further expands the number of states evaluated during planning. Despite this expense, the resulting policy is substantially stronger and far more adaptive, particularly in the Constrained environment where long-term spatial reasoning is required. Importantly, while training is costly, inference after training is efficient and requires only a single network evaluation per move. This reflects a classic trade-off in reinforcement learning: higher upfront computation yields significantly improved decision quality.
% ==========================================
% SECTION 5: CONCLUSION
% ==========================================
\section{Conclusion}
\label{sec:conclusion}
\textit{[Member 5: Summarize the project. What did we learn? Future work: Train for longer, use deeper ResNet, dynamic constraints.]}

% --- References ---
\medskip
\small
\begin{thebibliography}{9}

% Add other references here
\bibitem{gayas2014}
Gayas, V. (2014). \textit{2048 Using Expectimax}. Miner School of Computer and Information Sciences, UMass Lowell. Available at: \url{https://www.cs.uml.edu/ecg/uploads/AIfall14/vignesh_gayas_2048_project.pdf}

\bibitem{fuchs2020}
Fuchs, S. (2020). \textit{2048 AI | Develop \& Fix}. Available at: \url{https://www.sam-fuchs.com/s/projects/2048/}

\bibitem{antonoglou2022}
Antonoglou, I., et al. (2022). Planning in Stochastic Environments with a Learned Model. \textit{International Conference on Learning Representations (ICLR)}. Available at: \url{https://openreview.net/pdf?id=X6D9bAHhBQ1}

\end{thebibliography}

\end{document}